# Summary
This project intends to reproduce the results in the paper “Logical Fallacy Detection” (2022) by Jin, et al. The paper aims to perform the NLP task of detecting specific types of logical fallacies in text: in a dataset of general instances of logical fallacies, as well as an extrapolation challenge dataset of instances of logical fallacies in climate change claims. A classifier was trained on the first dataset, and its results on these datasets were compared with that of existing large language models. The paper can be found here: https://aclanthology.org/2022.findings-emnlp.532/. The paper also links a repository with code to train/use the models, the datasets, and some of the saved models.
# MVAP	
As for the minimal viable action plan to reproduce the paper’s results, these would be the steps that would need to be taken:
1.	We’d need to first familiarize ourselves fully with the paper and its corresponding codebase. This would involve closely reading both and working together to understand both of them fully, so we’re able to use their code properly. This can also involve trying to use the codebase, so that we’re more familiar with the workflow of the codebase, and to ensure that we can execute the codebase’s intended behaviors properly (make sure things work on our end, as well as make sure we can access the large language models that they used in their evaluations). We estimate this step to take about 1.5 to 2 weeks.
2.	Then, we’d try to reproduce the paper’s results. For this paper, this would involve evaluating the various models (that were evaluated in the paper) on the paper’s datasets, using the same metrics they used. We would see if our evaluation results lined up with theirs. Specifically, we want to see if the models that they say are the best perform as well as they said, and we want to see if those models outperform the others. The repository has some, but not all, of the saved models that were used in the paper. Some of the models they evaluated but didn’t include in the repository were large language models fine-tuned on their datasets. This would mean that we’d have to obtain these models and do this fine-tuning too. We estimate this step to take about 2 to 2.5 weeks, depending on how long the fine-tuning of the models take.
3.	Then, if we successfully reproduced the paper’s results, then we’d perform an additional experiment not in the paper. One possible experiment would be to see how the model performs on valid arguments (does the model falsely detect a fallacy in a valid argument?). We estimate this step to take about 1.5 to 2 weeks, depending on what exactly the experiment is.
# Stretch Goals
If we complete the minimal viable action plan with time to spare, these would be some stretch goals we can go for:
•	Try to improve the model’s performance via changing the best model’s hyperparameters.
•	See how GPT-3.5 performs on these datasets. Although this would involve obtaining the ability to use and fine-tune GPT-3.5 (which might cost money?).
•	Do the same for GPT-4.